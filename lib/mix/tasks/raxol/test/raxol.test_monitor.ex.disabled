defmodule Mix.Tasks.Raxol.TestMonitor do
  @moduledoc """
  Test performance monitoring infrastructure for Raxol.

  Tracks test execution times, identifies slow tests, detects performance
  regressions, and generates performance reports. Uses real test execution
  data with no simulation or mocking.

  ## Usage

      mix raxol.test_monitor [options]
      
  ## Options

  * `--threshold` - Slow test threshold in milliseconds (default: 1000)
  * `--baseline` - Path to baseline performance file
  * `--output` - Output format: json, csv, or table (default: table)
  * `--save` - Save results to performance history
  * `--compare` - Compare against previous run
  * `--watch` - Watch mode for continuous monitoring
  """

  use Mix.Task

  @shortdoc "Monitor test performance and detect regressions"

  @default_threshold 1000
  @performance_dir ".tmp/test_performance"
  @history_file "#{@performance_dir}/history.json"

  def run(args) do
    {opts, _, _} =
      OptionParser.parse(args,
        switches: [
          threshold: :integer,
          baseline: :string,
          output: :string,
          save: :boolean,
          compare: :boolean,
          watch: :boolean
        ],
        aliases: [
          t: :threshold,
          b: :baseline,
          o: :output,
          s: :save,
          c: :compare,
          w: :watch
        ]
      )

    threshold = opts[:threshold] || @default_threshold
    output_format = opts[:output] || "table"

    Mix.shell().info("Starting test performance monitoring...")
    Mix.shell().info("Slow test threshold: #{threshold}ms")

    if opts[:watch] do
      run_watch_mode(threshold, output_format, opts)
    else
      run_single_analysis(threshold, output_format, opts)
    end
  end

  defp run_single_analysis(threshold, output_format, opts) do
    ensure_performance_dir()

    # Run tests with timing
    {results, total_time} = measure_test_execution()

    # Analyze performance
    analysis = analyze_performance(results, threshold)

    # Generate report
    report = generate_report(analysis, total_time, output_format)

    # Save to history if requested
    if opts[:save] do
      save_to_history(analysis, total_time)
    end

    # Compare with baseline or previous run
    report = 
      if opts[:compare] or opts[:baseline] do
        comparison = compare_performance(analysis, opts[:baseline])
        add_comparison_to_report(report, comparison)
      else
        report
      end

    # Output report
    output_report(report, output_format)

    # Exit with appropriate code
    if analysis.regressions > 0 do
      Mix.shell().error("âŒ Performance regressions detected!")
      System.halt(1)
    else
      Mix.shell().info("âœ… No performance regressions detected")
    end
  end

  defp run_watch_mode(threshold, output_format, opts) do
    Mix.shell().info("Entering watch mode... Press Ctrl+C to exit")

    spawn(fn ->
      watch_loop(threshold, output_format, opts)
    end)

    # Keep the main process alive
    :timer.sleep(:infinity)
  end

  defp watch_loop(threshold, output_format, opts) do
    try do
      run_single_analysis(threshold, output_format, opts)
    rescue
      e ->
        Mix.shell().error("Error during analysis: #{inspect(e)}")
    catch
      :exit, _ ->
        Mix.shell().info("Test execution failed, retrying in 5s...")
    end

    # Wait 5 seconds before next run
    :timer.sleep(5000)
    watch_loop(threshold, output_format, opts)
  end

  defp measure_test_execution do
    start_time = System.monotonic_time(:millisecond)

    # Use the standard test command with performance tracking
    test_args = [
      "test",
      "--exclude",
      "slow",
      "--exclude",
      "integration",
      "--exclude",
      "docker",
      "--formatter",
      "Raxol.TestMonitor.Formatter"
    ]

    env = [
      {"SKIP_TERMBOX2_TESTS", "true"},
      {"MIX_ENV", "test"},
      {"TMPDIR", "/tmp"},
      {"RAXOL_TEST_MONITOR", "true"}
    ]

    {output, _exit_code} =
      System.cmd("mix", test_args,
        env: env,
        into: [],
        stderr_to_stdout: true
      )

    end_time = System.monotonic_time(:millisecond)
    total_time = end_time - start_time

    # Parse test results from output
    results = parse_test_output(output)

    {results, total_time}
  end

  defp parse_test_output(output) do
    # Parse ExUnit output to extract timing information
    lines = Enum.join(output, "\\n") |> String.split("\\n")

    Enum.reduce(lines, {[], nil}, fn line, {acc, current_module} ->
      cond do
        # Module start
        String.contains?(line, "Test started for") ->
          module = extract_module_name(line)
          {[{:module_start, module, System.monotonic_time(:microsecond)} | acc], module}

        # Individual test result
        String.match?(line, ~r/\\s+\\*\\s+test\\s+.*\\s+\\(\\d+\\.\\d+ms\\)/) ->
          test_info = parse_test_line(line)
          {[test_info | acc], current_module}

        # Module finish
        String.contains?(line, "Finished in") ->
          time_info = parse_finish_line(line)
          {[time_info | acc], current_module}

        true ->
          {acc, current_module}
      end
    end)
    |> elem(0)
    |> Enum.reverse()
    |> group_by_module()
  end

  defp extract_module_name(line) do
    case Regex.run(~r/Test started for (.+)/, line) do
      [_, module] -> module
      _ -> "Unknown"
    end
  end

  defp parse_test_line(line) do
    # Extract test name and timing from ExUnit output
    case Regex.run(~r/\\s+\\*\\s+test\\s+(.+?)\\s+\\((\\d+\\.\\d+)ms\\)/, line) do
      [_, test_name, time_str] ->
        time = String.to_float(time_str)
        status = if String.contains?(line, "PASS"), do: :passed, else: :failed

        %{
          name: String.trim(test_name),
          time_ms: time,
          status: status,
          timestamp: System.system_time(:millisecond)
        }

      _ ->
        %{
          name: "unknown",
          time_ms: 0,
          status: :unknown,
          timestamp: System.system_time(:millisecond)
        }
    end
  end

  defp parse_finish_line(line) do
    case Regex.run(~r/Finished in (\\d+\\.\\d+) seconds/, line) do
      [_, time_str] ->
        total_seconds = String.to_float(time_str)
        %{total_time_ms: total_seconds * 1000}

      _ ->
        %{total_time_ms: 0}
    end
  end

  defp group_by_module(results) do
    # Group test results by module for better analysis
    results
    |> Enum.reduce(%{}, fn result, acc ->
      case result do
        %{name: test_name} = test ->
          # Extract module from test name or use current module
          module = extract_test_module(test_name)
          tests = Map.get(acc, module, [])
          Map.put(acc, module, [test | tests])

        _ ->
          acc
      end
    end)
  end

  defp extract_test_module(test_name) do
    # Try to extract module from fully qualified test names
    case String.split(test_name, ".") do
      [module | _] when byte_size(module) > 0 -> module
      _ -> "Unknown"
    end
  end

  defp analyze_performance(results, threshold) do
    all_tests = results |> Map.values() |> List.flatten()

    slow_tests = Enum.filter(all_tests, &(&1.time_ms > threshold))
    failed_tests = Enum.filter(all_tests, &(&1.status == :failed))

    # Calculate statistics
    times = Enum.map(all_tests, & &1.time_ms)

    avg_time =
      if length(times) > 0, do: Enum.sum(times) / length(times), else: 0

    max_time = if length(times) > 0, do: Enum.max(times), else: 0
    min_time = if length(times) > 0, do: Enum.min(times), else: 0

    # Detect performance patterns
    performance_trends = analyze_trends(all_tests)
    bottlenecks = identify_bottlenecks(results, threshold)

    %{
      total_tests: length(all_tests),
      slow_tests: length(slow_tests),
      failed_tests: length(failed_tests),
      avg_time_ms: avg_time,
      max_time_ms: max_time,
      min_time_ms: min_time,
      slow_test_details: slow_tests,
      failed_test_details: failed_tests,
      performance_trends: performance_trends,
      bottlenecks: bottlenecks,
      # Will be set by comparison
      regressions: 0
    }
  end

  defp analyze_trends(tests) do
    # Analyze performance trends over time
    sorted_tests = Enum.sort_by(tests, & &1.timestamp)

    if length(sorted_tests) < 2 do
      %{trend: :insufficient_data}
    else
      times = Enum.map(sorted_tests, & &1.time_ms)

      # Simple linear regression to detect trends
      {slope, _intercept} = linear_regression(times)

      trend =
        cond do
          slope > 0.1 -> :degrading
          slope < -0.1 -> :improving
          true -> :stable
        end

      %{
        trend: trend,
        slope: slope,
        sample_size: length(times)
      }
    end
  end

  defp linear_regression(values) do
    n = length(values)
    indices = 1..n |> Enum.to_list()

    sum_x = Enum.sum(indices)
    sum_y = Enum.sum(values)

    sum_xy =
      Enum.zip(indices, values)
      |> Enum.map(fn {x, y} -> x * y end)
      |> Enum.sum()

    sum_xx = Enum.map(indices, &(&1 * &1)) |> Enum.sum()

    slope = (n * sum_xy - sum_x * sum_y) / (n * sum_xx - sum_x * sum_x)
    intercept = (sum_y - slope * sum_x) / n

    {slope, intercept}
  end

  defp identify_bottlenecks(results, threshold) do
    # Identify modules/patterns that consistently have slow tests
    module_stats =
      Enum.map(results, fn {module, tests} ->
        slow_count = Enum.count(tests, &(&1.time_ms > threshold))
        avg_time = Enum.sum(Enum.map(tests, & &1.time_ms)) / length(tests)

        %{
          module: module,
          total_tests: length(tests),
          slow_tests: slow_count,
          avg_time_ms: avg_time,
          bottleneck_score: slow_count / length(tests)
        }
      end)

    # Sort by bottleneck score
    Enum.sort_by(module_stats, & &1.bottleneck_score, :desc)
    # Top 5 bottlenecks
    |> Enum.take(5)
  end

  defp generate_report(analysis, total_time, output_format) do
    %{
      timestamp: System.system_time(:millisecond),
      total_execution_time_ms: total_time,
      analysis: analysis,
      format: output_format
    }
  end

  defp compare_performance(current_analysis, baseline_path) do
    baseline = load_baseline(baseline_path)

    if baseline do
      %{
        regression_count: count_regressions(current_analysis, baseline),
        improved_count: count_improvements(current_analysis, baseline),
        avg_time_change: current_analysis.avg_time_ms - baseline.avg_time_ms,
        slow_test_change: current_analysis.slow_tests - baseline.slow_tests
      }
    else
      %{error: "No baseline available for comparison"}
    end
  end

  defp load_baseline(nil) do
    # Load most recent from history
    if File.exists?(@history_file) do
      case File.read(@history_file) do
        {:ok, content} ->
          case Jason.decode(content) do
            {:ok, history} when is_list(history) and length(history) > 0 ->
              List.last(history)

            _ ->
              nil
          end

        _ ->
          nil
      end
    else
      nil
    end
  end

  defp load_baseline(path) do
    if File.exists?(path) do
      case File.read(path) do
        {:ok, content} ->
          case Jason.decode(content) do
            {:ok, data} -> data
            _ -> nil
          end

        _ ->
          nil
      end
    else
      nil
    end
  end

  defp count_regressions(current, baseline) do
    # Count tests that got significantly slower
    current_tests = Map.new(current.slow_test_details, &{&1.name, &1.time_ms})

    baseline_tests =
      Map.new(
        Map.get(baseline, "slow_test_details", []),
        &{Map.get(&1, "name"), Map.get(&1, "time_ms", 0)}
      )

    Enum.count(current_tests, fn {test_name, current_time} ->
      baseline_time = Map.get(baseline_tests, test_name, 0)
      # 20% threshold
      baseline_time > 0 and current_time > baseline_time * 1.2
    end)
  end

  defp count_improvements(current, baseline) do
    # Count tests that got significantly faster
    current_tests = Map.new(current.slow_test_details, &{&1.name, &1.time_ms})

    baseline_tests =
      Map.new(
        Map.get(baseline, "slow_test_details", []),
        &{Map.get(&1, "name"), Map.get(&1, "time_ms", 0)}
      )

    Enum.count(baseline_tests, fn {test_name, baseline_time} ->
      current_time = Map.get(current_tests, test_name, 0)
      # 20% threshold
      current_time > 0 and current_time < baseline_time * 0.8
    end)
  end

  defp add_comparison_to_report(report, comparison) do
    Map.put(report, :comparison, comparison)
  end

  defp output_report(report, "json") do
    json = Jason.encode!(report, pretty: true)
    IO.puts(json)
  end

  defp output_report(report, "csv") do
    output_csv_report(report)
  end

  defp output_report(report, _) do
    output_table_report(report)
  end

  defp output_table_report(report) do
    analysis = report.analysis

    Mix.shell().info("")
    Mix.shell().info("ðŸ” Test Performance Report")
    Mix.shell().info("=" <> String.duplicate("=", 50))
    Mix.shell().info("")

    Mix.shell().info("ðŸ“Š Summary Statistics")
    Mix.shell().info("  Total Tests: #{analysis.total_tests}")
    Mix.shell().info("  Slow Tests: #{analysis.slow_tests}")
    Mix.shell().info("  Failed Tests: #{analysis.failed_tests}")

    Mix.shell().info(
      "  Average Time: #{Float.round(analysis.avg_time_ms, 2)}ms"
    )

    Mix.shell().info("  Max Time: #{Float.round(analysis.max_time_ms, 2)}ms")

    Mix.shell().info(
      "  Total Execution: #{Float.round(report.total_execution_time_ms / 1000, 2)}s"
    )

    if length(analysis.slow_test_details) > 0 do
      Mix.shell().info("")
      Mix.shell().info("ðŸŒ Slowest Tests")

      analysis.slow_test_details
      |> Enum.sort_by(& &1.time_ms, :desc)
      |> Enum.take(10)
      |> Enum.with_index(1)
      |> Enum.each(fn {test, index} ->
        Mix.shell().info(
          "  #{index}. #{test.name} (#{Float.round(test.time_ms, 2)}ms)"
        )
      end)
    end

    if length(analysis.bottlenecks) > 0 do
      Mix.shell().info("")
      Mix.shell().info("ðŸ”¥ Performance Bottlenecks")

      Enum.each(analysis.bottlenecks, fn bottleneck ->
        percentage = Float.round(bottleneck.bottleneck_score * 100, 1)
        avg_time = Float.round(bottleneck.avg_time_ms, 2)

        Mix.shell().info(
          "  #{bottleneck.module}: #{percentage}% slow tests (#{avg_time}ms avg)"
        )
      end)
    end

    # Trends analysis
    case analysis.performance_trends.trend do
      :degrading ->
        Mix.shell().error("")
        Mix.shell().error("âš ï¸  Performance Trend: DEGRADING")

      :improving ->
        Mix.shell().info("")
        Mix.shell().info("âœ… Performance Trend: IMPROVING")

      :stable ->
        Mix.shell().info("")
        Mix.shell().info("âž¡ï¸  Performance Trend: STABLE")

      _ ->
        nil
    end

    # Comparison results
    if Map.has_key?(report, :comparison) do
      comparison = report.comparison

      Mix.shell().info("")
      Mix.shell().info("ðŸ“ˆ Performance Comparison")

      if Map.has_key?(comparison, :error) do
        Mix.shell().info("  #{comparison.error}")
      else
        Mix.shell().info("  Regressions: #{comparison.regression_count}")
        Mix.shell().info("  Improvements: #{comparison.improved_count}")

        Mix.shell().info(
          "  Avg Time Change: #{Float.round(comparison.avg_time_change, 2)}ms"
        )

        Mix.shell().info("  Slow Test Change: #{comparison.slow_test_change}")
      end
    end

    Mix.shell().info("")
  end

  defp output_csv_report(report) do
    analysis = report.analysis

    # Header
    IO.puts("test_name,time_ms,status,module")

    # Test data
    all_tests =
      (analysis.slow_test_details ++ analysis.failed_test_details)
      |> Enum.uniq_by(& &1.name)

    Enum.each(all_tests, fn test ->
      module = extract_test_module(test.name)
      IO.puts("#{test.name},#{test.time_ms},#{test.status},#{module}")
    end)
  end

  defp save_to_history(analysis, total_time) do
    ensure_performance_dir()

    entry = %{
      timestamp: System.system_time(:millisecond),
      total_time_ms: total_time,
      analysis: analysis
    }

    history = load_history()
    # Keep last 100 entries
    updated_history = [entry | history] |> Enum.take(100)

    case File.write(@history_file, Jason.encode!(updated_history, pretty: true)) do
      :ok ->
        Mix.shell().info("Performance data saved to history")

      {:error, reason} ->
        Mix.shell().error("Failed to save performance data: #{reason}")
    end
  end

  defp load_history do
    if File.exists?(@history_file) do
      case File.read(@history_file) do
        {:ok, content} ->
          case Jason.decode(content) do
            {:ok, history} when is_list(history) -> history
            _ -> []
          end

        _ ->
          []
      end
    else
      []
    end
  end

  defp ensure_performance_dir do
    File.mkdir_p!(@performance_dir)
  end
end
