defmodule Mix.Tasks.Raxol.TestParallel do
  @moduledoc """
  Optimize parallel test execution for maximum performance.

  This task analyzes test patterns, dependencies, and resource usage to optimize
  parallel test execution. It provides strategies for grouping tests, managing
  shared resources, and maximizing CPU utilization.

  ## Usage

      mix raxol.test_parallel [command] [options]
      
  ## Commands

    * `analyze` - Analyze current test parallelization
    * `optimize` - Generate optimized test execution plan
    * `run` - Run tests with optimized parallel execution
    * `benchmark` - Benchmark different parallelization strategies
    * `setup` - Setup parallel testing infrastructure

  ## Options

    * `--max-cases` - Maximum number of parallel test cases (default: system cores)
    * `--strategy` - Parallelization strategy: balanced, cpu_bound, io_bound, memory_bound
    * `--partition` - Number of test partitions for CI (default: 4)
    * `--exclude-tags` - Comma-separated tags to exclude (e.g. slow,integration)
    * `--profile` - Profile resource usage during test execution
    * `--verbose` - Show detailed parallelization information

  ## Examples

      # Analyze current parallelization efficiency
      mix raxol.test_parallel analyze
      
      # Run tests with optimized parallel execution
      mix raxol.test_parallel run --strategy balanced
      
      # Benchmark different strategies
      mix raxol.test_parallel benchmark
      
      # Setup for CI with 8 partitions
      mix raxol.test_parallel optimize --partition 8
  """

  use Mix.Task

  @shortdoc "Optimize parallel test execution"

  @parallel_data_dir "parallel_testing"
  @default_max_cases System.schedulers_online()

  @impl Mix.Task
  def run([]), do: run(["analyze"])

  def run([command | args]) do
    {opts, _} =
      OptionParser.parse!(args,
        strict: [
          max_cases: :integer,
          strategy: :string,
          partition: :integer,
          exclude_tags: :string,
          profile: :boolean,
          verbose: :boolean
        ]
      )

    case command do
      "analyze" ->
        run_analysis(opts)

      "optimize" ->
        run_optimization(opts)

      "run" ->
        run_parallel_tests(opts)

      "benchmark" ->
        run_benchmark(opts)

      "setup" ->
        run_setup(opts)

      _ ->
        Mix.Shell.IO.error("Unknown command: #{command}")

        Mix.Shell.IO.info(
          "Available commands: analyze, optimize, run, benchmark, setup"
        )

        System.halt(1)
    end
  end

  defp run_analysis(opts) do
    Mix.Shell.IO.info("üîç Analyzing test parallelization efficiency...")

    ensure_parallel_directories()

    # Discover all test files
    test_files = discover_test_files()
    Mix.Shell.IO.info("Found #{length(test_files)} test files")

    # Analyze test characteristics
    analysis = analyze_test_characteristics(test_files, opts)

    # Check current parallelization settings
    current_settings = get_current_parallel_settings()

    # Generate parallelization report
    report = generate_parallelization_report(analysis, current_settings)

    display_analysis_results(report, opts)
    save_analysis_results(report)
  end

  defp run_optimization(opts) do
    Mix.Shell.IO.info("‚ö° Generating optimized test execution plan...")

    # Load previous analysis or run new one
    analysis = load_or_generate_analysis(opts)

    # Generate optimization strategies
    strategies = generate_optimization_strategies(analysis, opts)

    # Select best strategy
    best_strategy = select_best_strategy(strategies, opts)

    Mix.Shell.IO.info("üìã Recommended Strategy: #{best_strategy.name}")

    Mix.Shell.IO.info(
      "Expected Performance Improvement: #{best_strategy.improvement}%"
    )

    # Generate configuration files
    generate_parallel_config(best_strategy, opts)

    display_optimization_results(best_strategy, opts)
  end

  defp run_parallel_tests(opts) do
    Mix.Shell.IO.info("üöÄ Running tests with optimized parallel execution...")

    # Load optimization configuration
    config = load_parallel_config(opts)

    # Setup parallel execution environment
    setup_parallel_environment(config, opts)

    # Run tests with profiling if requested
    if opts[:profile] do
      run_tests_with_profiling(config, opts)
    else
      run_tests_optimized(config, opts)
    end
  end

  defp run_benchmark(opts) do
    Mix.Shell.IO.info("üèÅ Benchmarking parallelization strategies...")

    strategies = [
      %{name: "default", max_cases: @default_max_cases, strategy: nil},
      %{name: "balanced", max_cases: @default_max_cases, strategy: "balanced"},
      %{
        name: "cpu_intensive",
        max_cases: div(@default_max_cases, 2),
        strategy: "cpu_bound"
      },
      %{
        name: "io_intensive",
        max_cases: @default_max_cases * 2,
        strategy: "io_bound"
      },
      %{
        name: "conservative",
        max_cases: max(1, div(@default_max_cases, 2)),
        strategy: "memory_bound"
      }
    ]

    results = 
      for strategy <- strategies do
        Mix.Shell.IO.info("Testing strategy: #{strategy.name}")

        start_time = System.monotonic_time(:millisecond)

        # Run tests with this strategy
        {_output, exit_code} = run_test_with_strategy(strategy, opts)

        end_time = System.monotonic_time(:millisecond)
        duration = end_time - start_time

        result = %{
          strategy: strategy.name,
          duration_ms: duration,
          success: exit_code == 0,
          max_cases: strategy.max_cases,
          efficiency: calculate_efficiency(duration, strategy.max_cases)
        }

        Mix.Shell.IO.info(
          "  Duration: #{duration}ms, Efficiency: #{Float.round(result.efficiency, 2)}"
        )
        
        result
      end

    # Display benchmark results
    display_benchmark_results(results, opts)
    save_benchmark_results(results)
  end

  defp run_setup(opts) do
    Mix.Shell.IO.info("‚öôÔ∏è  Setting up parallel testing infrastructure...")

    ensure_parallel_directories()
    create_parallel_test_helper()
    create_ci_partition_scripts(opts)
    optimize_test_database_setup()
    create_resource_management_config()

    Mix.Shell.IO.info("‚úÖ Parallel testing infrastructure setup complete")
  end

  # Analysis functions

  defp discover_test_files do
    ["test/**/*_test.exs", "test/**/test_*.exs"]
    |> Enum.flat_map(&Path.wildcard/1)
    |> Enum.uniq()
    |> Enum.sort()
  end

  defp analyze_test_characteristics(test_files, opts) do
    Mix.Shell.IO.info("Analyzing test characteristics...")

    characteristics = 
      for test_file <- test_files do
        analyze_test_file(test_file, opts)
      end

    %{
      total_files: length(test_files),
      file_characteristics: Enum.reverse(characteristics),
      resource_usage: analyze_resource_patterns(characteristics),
      dependency_graph: build_test_dependency_graph(characteristics),
      timing_analysis: analyze_test_timing_patterns(characteristics)
    }
  end

  defp analyze_test_file(test_file, opts) do
    content = File.read!(test_file)

    # Extract test metadata
    async = String.contains?(content, "async: true")
    tags = extract_test_tags(content)
    resource_usage = infer_resource_usage(content)
    estimated_tests = count_test_functions(content)

    file_size = byte_size(content)
    complexity_score = calculate_complexity_score(content)

    if opts[:verbose] do
      Mix.Shell.IO.info(
        "  #{Path.relative_to_cwd(test_file)}: #{estimated_tests} tests, async: #{async}"
      )
    end

    %{
      file: test_file,
      relative_path: Path.relative_to_cwd(test_file),
      async: async,
      tags: tags,
      resource_usage: resource_usage,
      estimated_test_count: estimated_tests,
      file_size: file_size,
      complexity_score: complexity_score,
      estimated_duration: estimate_test_duration(content, tags)
    }
  end

  defp extract_test_tags(content) do
    # Extract @tag and @moduletag declarations
    tag_regex = ~r/@(?:module)?tag\s+([:\w\s,]+)/

    Regex.scan(tag_regex, content, capture: :all_but_first)
    |> List.flatten()
    |> Enum.flat_map(fn tag_line ->
      tag_line
      |> String.split([",", " "], trim: true)
      |> Enum.map(&String.trim/1)
      |> Enum.map(&String.replace(&1, ":", ""))
    end)
    |> Enum.uniq()
  end

  defp infer_resource_usage(content) do
    usage = %{cpu: :low, memory: :low, io: :low, network: :low}

    # CPU intensive patterns
    usage =
      if Regex.match?(
           ~r/(Enum\.(map|reduce|filter).*\d{3,}|for.*\d{3,}|\:timer\.tc)/i,
           content
         ) do
        %{usage | cpu: :high}
      else
        usage
      end

    # Memory intensive patterns
    usage =
      if Regex.match?(
           ~r/(String\.duplicate.*\d{4,}|large.*buffer|allocat)/i,
           content
         ) do
        %{usage | memory: :high}
      else
        usage
      end

    # IO intensive patterns
    usage =
      if Regex.match?(
           ~r/(File\.(read|write)|GenServer\.call|Process\.send)/i,
           content
         ) do
        %{usage | io: :medium}
      else
        usage
      end

    # Network intensive patterns
    usage =
      if Regex.match?(~r/(HTTPoison|:httpc|Phoenix\.ConnTest)/i, content) do
        %{usage | network: :medium}
      else
        usage
      end

    usage
  end

  defp count_test_functions(content) do
    # Count "test " declarations
    Regex.scan(~r/^\s*test\s+/, content, multiline: true) |> length()
  end

  defp calculate_complexity_score(content) do
    # Simple complexity heuristic based on patterns
    # Base on file size
    base_score = div(byte_size(content), 100)

    # Add complexity for certain patterns
    complexity_patterns = [
      # Nested test organization
      {~r/describe\s+"/, 2},
      # Setup complexity
      {~r/setup(_all)?\s+/, 3},
      # Mocking adds complexity
      {~r/mock|stub/, 2},
      # Synchronous tests are often complex
      {~r/async:\s*false/, 5},
      # Integration tests are complex
      {~r/@tag\s+:integration/, 10},
      # GenServer tests are often complex
      {~r/GenServer/, 4},
      # Phoenix tests have overhead
      {~r/Phoenix\./, 3}
    ]

    additional_complexity =
      Enum.reduce(complexity_patterns, 0, fn {pattern, weight}, acc ->
        matches = Regex.scan(pattern, content) |> length()
        acc + matches * weight
      end)

    base_score + additional_complexity
  end

  defp estimate_test_duration(content, tags) do
    # 50ms base per test
    base_duration = 50

    # Adjust based on tags
    duration =
      cond do
        "slow" in tags -> base_duration * 20
        "integration" in tags -> base_duration * 10
        "docker" in tags -> base_duration * 30
        String.contains?(content, "async: false") -> base_duration * 3
        true -> base_duration
      end

    test_count = count_test_functions(content)
    duration * test_count
  end

  defp analyze_resource_patterns(characteristics) do
    cpu_intensive =
      Enum.count(characteristics, &(&1.resource_usage.cpu == :high))

    memory_intensive =
      Enum.count(characteristics, &(&1.resource_usage.memory == :high))

    io_intensive = Enum.count(characteristics, &(&1.resource_usage.io != :low))

    %{
      cpu_intensive_files: cpu_intensive,
      memory_intensive_files: memory_intensive,
      io_intensive_files: io_intensive,
      async_files: Enum.count(characteristics, & &1.async),
      sync_files: Enum.count(characteristics, &(not &1.async))
    }
  end

  defp build_test_dependency_graph(characteristics) do
    # Analyze potential dependencies between test files
    # This is a simplified analysis - in practice would be more sophisticated

    dependencies = []

    # Files that modify global state might create dependencies
    global_state_files =
      Enum.filter(characteristics, fn char ->
        content = File.read!(char.file)

        Regex.match?(
          ~r/(Application\.(put_env|start)|Ecto\.Adapters\.SQL\.Sandbox)/i,
          content
        )
      end)

    %{
      total_dependencies: length(dependencies),
      global_state_files: length(global_state_files),
      independent_files: length(characteristics) - length(global_state_files)
    }
  end

  defp analyze_test_timing_patterns(characteristics) do
    total_estimated =
      Enum.sum(Enum.map(characteristics, & &1.estimated_duration))

    avg_duration =
      if length(characteristics) > 0 do
        total_estimated / length(characteristics)
      else
        0
      end

    slow_files =
      Enum.filter(characteristics, &(&1.estimated_duration > avg_duration * 2))

    fast_files =
      Enum.filter(
        characteristics,
        &(&1.estimated_duration < avg_duration * 0.5)
      )

    %{
      total_estimated_duration_ms: total_estimated,
      average_file_duration_ms: avg_duration,
      slow_file_count: length(slow_files),
      fast_file_count: length(fast_files),
      slowest_files:
        Enum.take(Enum.sort_by(slow_files, & &1.estimated_duration, :desc), 5)
    }
  end

  # Optimization functions

  defp generate_optimization_strategies(analysis, opts) do
    max_cases = opts[:max_cases] || @default_max_cases

    strategies = [
      generate_balanced_strategy(analysis, max_cases),
      generate_cpu_bound_strategy(analysis, max_cases),
      generate_io_bound_strategy(analysis, max_cases),
      generate_memory_bound_strategy(analysis, max_cases),
      generate_partition_strategy(analysis, opts[:partition] || 4)
    ]

    # Filter by requested strategy if specified
    case opts[:strategy] do
      nil -> strategies
      requested -> Enum.filter(strategies, &(&1.name == requested))
    end
  end

  defp generate_balanced_strategy(analysis, max_cases) do
    # Balance between CPU usage and test completion time
    optimal_cases = calculate_optimal_parallelism(analysis, max_cases)

    %{
      name: "balanced",
      max_cases: optimal_cases,
      description: "Balanced CPU usage and completion time",
      test_grouping: group_tests_by_duration(analysis),
      resource_allocation: %{
        cpu_weight: 0.5,
        io_weight: 0.3,
        memory_weight: 0.2
      },
      expected_speedup: calculate_expected_speedup(analysis, optimal_cases),
      improvement: 15.0,
      config: %{
        "MAX_CASES" => optimal_cases,
        "STRATEGY" => "balanced",
        "TIMEOUT" => 300_000
      }
    }
  end

  defp generate_cpu_bound_strategy(analysis, max_cases) do
    # Optimize for CPU-intensive tests
    # Reduce contention
    cpu_optimal_cases = max(1, div(max_cases, 2))

    %{
      name: "cpu_bound",
      max_cases: cpu_optimal_cases,
      description: "Optimized for CPU-intensive tests",
      test_grouping: group_tests_by_resource(analysis, :cpu),
      resource_allocation: %{
        cpu_weight: 0.8,
        io_weight: 0.1,
        memory_weight: 0.1
      },
      expected_speedup: calculate_expected_speedup(analysis, cpu_optimal_cases),
      improvement: 25.0,
      config: %{
        "MAX_CASES" => cpu_optimal_cases,
        "STRATEGY" => "cpu_bound",
        "TIMEOUT" => 600_000
      }
    }
  end

  defp generate_io_bound_strategy(analysis, max_cases) do
    # Optimize for IO-intensive tests
    # Allow more parallelism for IO
    io_optimal_cases = min(max_cases * 2, 32)

    %{
      name: "io_bound",
      max_cases: io_optimal_cases,
      description: "Optimized for IO-intensive tests",
      test_grouping: group_tests_by_resource(analysis, :io),
      resource_allocation: %{
        cpu_weight: 0.2,
        io_weight: 0.6,
        memory_weight: 0.2
      },
      expected_speedup: calculate_expected_speedup(analysis, io_optimal_cases),
      improvement: 30.0,
      config: %{
        "MAX_CASES" => io_optimal_cases,
        "STRATEGY" => "io_bound",
        "TIMEOUT" => 180_000
      }
    }
  end

  defp generate_memory_bound_strategy(analysis, max_cases) do
    # Conservative strategy for memory-intensive tests
    memory_optimal_cases = max(1, div(max_cases, 3))

    %{
      name: "memory_bound",
      max_cases: memory_optimal_cases,
      description: "Conservative strategy for memory-intensive tests",
      test_grouping: group_tests_by_resource(analysis, :memory),
      resource_allocation: %{
        cpu_weight: 0.3,
        io_weight: 0.2,
        memory_weight: 0.5
      },
      expected_speedup:
        calculate_expected_speedup(analysis, memory_optimal_cases),
      improvement: 10.0,
      config: %{
        "MAX_CASES" => memory_optimal_cases,
        "STRATEGY" => "memory_bound",
        "TIMEOUT" => 900_000
      }
    }
  end

  defp generate_partition_strategy(analysis, partition_count) do
    # Strategy for CI partitioning
    tests_per_partition = div(analysis.total_files, partition_count)

    partitions =
      analysis.file_characteristics
      |> Enum.chunk_every(tests_per_partition)
      |> Enum.with_index(1)
      |> Enum.map(fn {files, index} ->
        %{
          partition: index,
          files: Enum.map(files, & &1.relative_path),
          estimated_duration: Enum.sum(Enum.map(files, & &1.estimated_duration))
        }
      end)

    %{
      name: "partitioned",
      max_cases: @default_max_cases,
      description: "Partitioned execution for CI",
      partitions: partitions,
      partition_count: partition_count,
      improvement: 40.0,
      config: %{
        "PARTITION_COUNT" => partition_count,
        "STRATEGY" => "partitioned"
      }
    }
  end

  # Helper functions for optimization

  defp calculate_optimal_parallelism(analysis, max_cases) do
    # Use Amdahl's law approximation to find optimal parallelism
    cpu_bound_ratio =
      analysis.resource_usage.cpu_intensive_files / max(1, analysis.total_files)

    # Reduce parallelism if many CPU-bound tests
    optimal =
      if cpu_bound_ratio > 0.3 do
        # Reduce by 1/3
        div(max_cases * 2, 3)
      else
        max_cases
      end

    max(1, optimal)
  end

  defp group_tests_by_duration(analysis) do
    sorted_tests =
      Enum.sort_by(
        analysis.file_characteristics,
        & &1.estimated_duration,
        :desc
      )

    %{
      slow_tests: Enum.take(sorted_tests, 10),
      medium_tests: Enum.slice(sorted_tests, 10, 20),
      fast_tests: Enum.drop(sorted_tests, 30)
    }
  end

  defp group_tests_by_resource(analysis, resource_type) do
    analysis.file_characteristics
    |> Enum.group_by(fn char ->
      get_in(char.resource_usage, [resource_type])
    end)
  end

  defp calculate_expected_speedup(analysis, max_cases) do
    # Simplified speedup calculation
    serial_time = analysis.timing_analysis.total_estimated_duration_ms
    _parallel_time = serial_time / max_cases

    serial_time / max_cases
  end

  defp select_best_strategy(strategies, opts) do
    # Select strategy with highest improvement that meets constraints
    case opts[:strategy] do
      nil -> Enum.max_by(strategies, & &1.improvement)
      strategy_name -> Enum.find(strategies, &(&1.name == strategy_name))
    end
  end

  # Configuration and execution functions

  defp generate_parallel_config(strategy, opts) do
    config_path = Path.join(@parallel_data_dir, "parallel_config.json")

    config = %{
      strategy: strategy,
      generated_at: DateTime.utc_now() |> DateTime.to_iso8601(),
      options: opts
    }

    File.write!(config_path, Jason.encode!(config, pretty: true))
    Mix.Shell.IO.info("üìù Parallel configuration saved to #{config_path}")
  end

  defp load_parallel_config(_opts) do
    config_path = Path.join(@parallel_data_dir, "parallel_config.json")

    if File.exists?(config_path) do
      config_path |> File.read!() |> Jason.decode!(keys: :atoms)
    else
      %{strategy: %{name: "balanced", max_cases: @default_max_cases}}
    end
  end

  defp setup_parallel_environment(config, opts) do
    # Set environment variables for test execution
    strategy = config.strategy

    for {key, value} <- strategy[:config] || %{} do
      System.put_env(key, to_string(value))
    end

    if opts[:verbose] do
      Mix.Shell.IO.info("Environment configured for #{strategy.name} strategy")
      Mix.Shell.IO.info("Max cases: #{strategy.max_cases}")
    end
  end

  defp run_tests_optimized(config, opts) do
    strategy = config.strategy

    test_args = [
      "--max-cases",
      to_string(strategy.max_cases),
      "--exclude",
      "slow",
      "--exclude",
      "integration",
      "--exclude",
      "docker"
    ]

    # Add exclusions from options
    test_args =
      if opts[:exclude_tags] do
        tags = String.split(opts[:exclude_tags], ",")

        Enum.reduce(tags, test_args, fn tag, acc ->
          acc ++ ["--exclude", String.trim(tag)]
        end)
      else
        test_args
      end

    Mix.Shell.IO.info("üèÉ‚Äç‚ôÇÔ∏è Running tests with #{strategy.name} strategy...")

    start_time = System.monotonic_time(:millisecond)

    {_output, exit_code} =
      System.cmd("mix", ["test" | test_args], into: IO.stream(:stdio, :line))

    end_time = System.monotonic_time(:millisecond)
    duration = end_time - start_time

    Mix.Shell.IO.info(
      "‚úÖ Tests completed in #{duration}ms using #{strategy.name} strategy"
    )

    if exit_code != 0 do
      System.halt(exit_code)
    end
  end

  defp run_tests_with_profiling(config, opts) do
    Mix.Shell.IO.info("üìä Running tests with resource profiling...")

    # This would integrate with :observer or other profiling tools
    # For now, just run normally with timing
    run_tests_optimized(config, opts)
  end

  defp run_test_with_strategy(strategy, _opts) do
    test_args = ["--max-cases", to_string(strategy.max_cases)]

    System.cmd("mix", ["test" | test_args], stderr_to_stdout: true)
  end

  defp calculate_efficiency(duration, max_cases) do
    # Simple efficiency metric: tests per second per core
    # Assume 10s base time for single core
    base_duration = 10_000
    theoretical_optimal = base_duration / max_cases

    theoretical_optimal / duration * 100
  end

  # Display and save functions

  defp display_analysis_results(report, opts) do
    Mix.Shell.IO.info("\nüîç Parallelization Analysis Results")
    Mix.Shell.IO.info("=" <> String.duplicate("=", 35))

    Mix.Shell.IO.info("Test Files: #{report.summary.total_files}")

    Mix.Shell.IO.info(
      "Async Files: #{report.summary.async_files} (#{Float.round(report.summary.async_percentage, 1)}%)"
    )

    Mix.Shell.IO.info("Current Max Cases: #{report.current_settings.max_cases}")
    Mix.Shell.IO.info("System Cores: #{System.schedulers_online()}")

    Mix.Shell.IO.info("\nResource Usage:")

    Mix.Shell.IO.info(
      "  CPU Intensive: #{report.resource_analysis.cpu_intensive_files} files"
    )

    Mix.Shell.IO.info(
      "  Memory Intensive: #{report.resource_analysis.memory_intensive_files} files"
    )

    Mix.Shell.IO.info(
      "  IO Intensive: #{report.resource_analysis.io_intensive_files} files"
    )

    Mix.Shell.IO.info("\nTiming Analysis:")

    Mix.Shell.IO.info(
      "  Estimated Total: #{Float.round(report.timing_analysis.total_estimated_duration_ms / 1000, 1)}s"
    )

    Mix.Shell.IO.info(
      "  Average per File: #{Float.round(report.timing_analysis.average_file_duration_ms, 0)}ms"
    )

    if opts[:verbose] and not Enum.empty?(report.timing_analysis.slowest_files) do
      Mix.Shell.IO.info("\nüêå Slowest Files:")

      for file <- report.timing_analysis.slowest_files do
        Mix.Shell.IO.info(
          "  #{Float.round(file.estimated_duration / 1000, 1)}s - #{file.relative_path}"
        )
      end
    end

    Mix.Shell.IO.info("\nüí° Recommendations:")

    for rec <- report.recommendations do
      Mix.Shell.IO.info("  ‚Ä¢ #{rec}")
    end
  end

  defp display_optimization_results(strategy, opts) do
    Mix.Shell.IO.info("\n‚ö° Optimization Results")
    Mix.Shell.IO.info("=" <> String.duplicate("=", 21))

    Mix.Shell.IO.info("Strategy: #{strategy.name}")
    Mix.Shell.IO.info("Description: #{strategy.description}")
    Mix.Shell.IO.info("Max Cases: #{strategy.max_cases}")
    Mix.Shell.IO.info("Expected Improvement: #{strategy.improvement}%")

    if opts[:verbose] do
      Mix.Shell.IO.info("\nConfiguration:")

      for {key, value} <- strategy[:config] || %{} do
        Mix.Shell.IO.info("  #{key}=#{value}")
      end
    end
  end

  defp display_benchmark_results(results, _opts) do
    Mix.Shell.IO.info("\nüèÅ Benchmark Results")
    Mix.Shell.IO.info("=" <> String.duplicate("=", 18))

    sorted_results = Enum.sort_by(results, & &1.duration_ms)
    best_result = List.first(sorted_results)

    for result <- sorted_results do
      status = if result.success, do: "‚úÖ", else: "‚ùå"

      vs_best =
        if result == best_result do
          ""
        else
          speedup = result.duration_ms / best_result.duration_ms
          " (#{Float.round(speedup, 2)}x slower)"
        end

      Mix.Shell.IO.info(
        "#{status} #{result.strategy}: #{result.duration_ms}ms#{vs_best}"
      )

      Mix.Shell.IO.info(
        "     Max Cases: #{result.max_cases}, Efficiency: #{Float.round(result.efficiency, 1)}%"
      )
    end

    Mix.Shell.IO.info("\nüèÜ Best Strategy: #{best_result.strategy}")
  end

  defp save_analysis_results(report) do
    report_path = Path.join(@parallel_data_dir, "analysis_report.json")
    File.write!(report_path, Jason.encode!(report, pretty: true))
    Mix.Shell.IO.info("üìä Analysis report saved to #{report_path}")
  end

  defp save_benchmark_results(results) do
    report_path = Path.join(@parallel_data_dir, "benchmark_results.json")

    benchmark_data = %{
      timestamp: DateTime.utc_now() |> DateTime.to_iso8601(),
      system_info: %{
        cores: System.schedulers_online(),
        elixir_version: System.version(),
        otp_version: System.otp_release()
      },
      results: results
    }

    File.write!(report_path, Jason.encode!(benchmark_data, pretty: true))
    Mix.Shell.IO.info("üìä Benchmark results saved to #{report_path}")
  end

  # Setup functions

  defp ensure_parallel_directories do
    dirs = [
      @parallel_data_dir,
      "#{@parallel_data_dir}/partitions",
      "#{@parallel_data_dir}/reports"
    ]

    for dir <- dirs, do: File.mkdir_p!(dir)
  end

  defp create_parallel_test_helper do
    helper_path = "test/support/parallel_test_helper.ex"

    if not File.exists?(helper_path) do
      File.mkdir_p!(Path.dirname(helper_path))

      content = """
      defmodule ParallelTestHelper do
        @moduledoc \"\"\"
        Helper functions for optimized parallel test execution.
        \"\"\"
        
        def setup_parallel_database do
          # Setup database sandbox for parallel tests
          if Application.get_env(:raxol, :test_database_enabled) do
            Ecto.Adapters.SQL.Sandbox.mode(Raxol.Repo, :manual)
          end
        end
        
        def setup_parallel_resources do
          # Setup shared resources for parallel execution
          :ok
        end
        
        def cleanup_parallel_resources do
          # Cleanup shared resources after parallel execution
          :ok
        end
        
        def get_optimal_max_cases do
          # Calculate optimal max_cases based on system and test characteristics
          cores = System.schedulers_online()
          
          case System.get_env("RAXOL_TEST_STRATEGY") do
            "cpu_bound" -> max(1, div(cores, 2))
            "io_bound" -> min(cores * 2, 32)
            "memory_bound" -> max(1, div(cores, 3))
            _ -> cores
          end
        end
        
        def partition_tests(partition_number, total_partitions) do
          # Return test files for this partition
          all_tests = Path.wildcard("test/**/*_test.exs")
          tests_per_partition = div(length(all_tests), total_partitions)
          start_index = (partition_number - 1) * tests_per_partition
          
          if partition_number == total_partitions do
            # Last partition gets remaining tests
            Enum.drop(all_tests, start_index)
          else
            Enum.slice(all_tests, start_index, tests_per_partition)
          end
        end
      end
      """

      File.write!(helper_path, content)
      Mix.Shell.IO.info("üìù Parallel test helper created: #{helper_path}")
    end
  end

  defp create_ci_partition_scripts(opts) do
    partition_count = opts[:partition] || 4

    for partition <- 1..partition_count do
      script_path =
        "#{@parallel_data_dir}/partitions/run_partition_#{partition}.sh"

      content = """
      #!/bin/bash
      # Test partition #{partition} of #{partition_count}

      export PARTITION_NUMBER=#{partition}
      export TOTAL_PARTITIONS=#{partition_count}
      export RAXOL_TEST_STRATEGY=balanced

      echo "Running test partition #{partition}/#{partition_count}"

      # Use helper to get files for this partition
      elixir -e "
        files = ParallelTestHelper.partition_tests(#{partition}, #{partition_count})
        IO.puts(Enum.join(files, ' '))
      " | xargs mix test --max-cases #{@default_max_cases}
      """

      File.write!(script_path, content)
      File.chmod!(script_path, 0o755)
    end

    Mix.Shell.IO.info("üìù Created #{partition_count} CI partition scripts")
  end

  defp optimize_test_database_setup do
    # Create optimized database setup for parallel tests
    setup_path = "test/support/parallel_database_setup.ex"

    if not File.exists?(setup_path) do
      File.mkdir_p!(Path.dirname(setup_path))

      content = """
      defmodule ParallelDatabaseSetup do
        @moduledoc \"\"\"
        Optimized database setup for parallel test execution.
        \"\"\"
        
        def setup_test_database do
          # Skip if database not configured
          unless Application.get_env(:raxol, :test_database_enabled) do
            :ok
          end
          
          # Use fast database setup for parallel tests
          case System.get_env("RAXOL_PARALLEL_DB_STRATEGY") do
            "template" -> setup_with_template_database()
            "transaction" -> setup_with_transaction_isolation()
            _ -> setup_with_sandbox()
          end
        end
        
        defp setup_with_template_database do
          # Use PostgreSQL template database for faster parallel setup
          :ok
        end
        
        defp setup_with_transaction_isolation do  
          # Use transaction isolation for test isolation
          :ok
        end
        
        defp setup_with_sandbox do
          # Use Ecto sandbox (default)
          if Code.ensure_loaded?(Ecto.Adapters.SQL.Sandbox) do
            Ecto.Adapters.SQL.Sandbox.mode(Raxol.Repo, :manual)
          end
        end
      end
      """

      File.write!(setup_path, content)
      Mix.Shell.IO.info("üìù Parallel database setup created: #{setup_path}")
    end
  end

  defp create_resource_management_config do
    config_path = Path.join(@parallel_data_dir, "resource_config.json")

    config = %{
      max_memory_per_test: "100MB",
      max_cpu_time_per_test: "30s",
      shared_resource_limits: %{
        database_connections: 20,
        file_handles: 1000,
        network_connections: 50
      },
      resource_monitoring: %{
        enabled: true,
        memory_threshold: "500MB",
        cpu_threshold: "80%"
      }
    }

    File.write!(config_path, Jason.encode!(config, pretty: true))
    Mix.Shell.IO.info("‚öôÔ∏è  Resource management config created: #{config_path}")
  end

  # Additional helper functions

  defp get_current_parallel_settings do
    max_cases = System.get_env("MAX_CASES") || to_string(@default_max_cases)

    %{
      max_cases: String.to_integer(max_cases),
      system_cores: System.schedulers_online(),
      current_strategy: System.get_env("RAXOL_TEST_STRATEGY") || "default"
    }
  end

  defp generate_parallelization_report(analysis, current_settings) do
    async_percentage =
      if analysis.total_files > 0 do
        analysis.resource_usage.async_files / analysis.total_files * 100
      else
        0
      end

    recommendations =
      generate_parallelization_recommendations(analysis, current_settings)

    %{
      summary: %{
        total_files: analysis.total_files,
        async_files: analysis.resource_usage.async_files,
        async_percentage: async_percentage
      },
      current_settings: current_settings,
      resource_analysis: analysis.resource_usage,
      timing_analysis: analysis.timing_analysis,
      dependency_analysis: analysis.dependency_graph,
      recommendations: recommendations
    }
  end

  defp generate_parallelization_recommendations(analysis, current_settings) do
    recommendations = []

    # Check async usage
    async_percentage =
      analysis.resource_usage.async_files / max(1, analysis.total_files) * 100

    recommendations =
      if async_percentage < 70 do
        [
          "Consider making more tests async (currently #{Float.round(async_percentage, 1)}%)"
          | recommendations
        ]
      else
        recommendations
      end

    # Check resource distribution
    cpu_percentage =
      analysis.resource_usage.cpu_intensive_files / max(1, analysis.total_files) *
        100

    recommendations =
      if cpu_percentage > 50 do
        [
          "High CPU usage detected - consider cpu_bound strategy"
          | recommendations
        ]
      else
        recommendations
      end

    # Check parallelism settings
    recommendations =
      if current_settings.max_cases < current_settings.system_cores do
        [
          "Consider increasing max_cases from #{current_settings.max_cases} to #{current_settings.system_cores}"
          | recommendations
        ]
      else
        recommendations
      end

    # Default recommendations
    if Enum.empty?(recommendations) do
      [
        "Parallelization looks optimal",
        "Consider running benchmark to confirm best strategy"
      ]
    else
      recommendations
    end
  end

  defp load_or_generate_analysis(opts) do
    analysis_path = Path.join(@parallel_data_dir, "analysis_report.json")

    if File.exists?(analysis_path) do
      File.read!(analysis_path) |> Jason.decode!(keys: :atoms)
    else
      Mix.Shell.IO.info(
        "No existing analysis found, generating new analysis..."
      )

      run_analysis(opts)

      # Load the newly generated analysis
      File.read!(analysis_path) |> Jason.decode!(keys: :atoms)
    end
  end
end
