name: Performance Regression Testing

on:
  push:
    branches: [ master, main ]
  pull_request:
    branches: [ master, main ]
  # Run weekly to establish baseline trends
  schedule:
    - cron: '0 2 * * 1'  # Monday 2 AM UTC
  workflow_dispatch:
    inputs:
      baseline_ref:
        description: 'Git reference to use as baseline (default: master)'
        required: false
        default: 'master'

env:
  SKIP_TERMBOX2_TESTS: true
  TMPDIR: /tmp
  MIX_ENV: test

jobs:
  performance-regression:
    runs-on: ubuntu-latest
    timeout-minutes: 30

    strategy:
      fail-fast: false
      matrix:
        elixir: ['1.17.1']
        otp: ['25.3.2.7']

    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Need full history for comparison

    - name: Set up Elixir
      uses: erlef/setup-beam@v1
      with:
        elixir-version: ${{ matrix.elixir }}
        otp-version: ${{ matrix.otp }}

    - name: Cache dependencies
      uses: actions/cache@v4
      with:
        path: |
          _build
          deps
          ~/.cache/rebar3
          ~/.hex
        key: deps-${{ runner.os }}-${{ matrix.otp }}-${{ matrix.elixir }}-${{ hashFiles('mix.lock') }}
        restore-keys: |
          deps-${{ runner.os }}-${{ matrix.otp }}-${{ matrix.elixir }}-

    - name: Cache PLT files
      uses: actions/cache@v4
      with:
        path: priv/plts
        key: plt-${{ runner.os }}-${{ matrix.otp }}-${{ matrix.elixir }}-${{ hashFiles('mix.lock') }}
        restore-keys: |
          plt-${{ runner.os }}-${{ matrix.otp }}-${{ matrix.elixir }}-

    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y build-essential gcc make
        echo "TMPDIR=/tmp" >> $GITHUB_ENV

    - name: Install dependencies
      run: |
        mix local.hex --force
        mix local.rebar --force
        mix deps.get

    - name: Compile
      run: |
        mix compile --warnings-as-errors

    - name: Setup performance baseline directory
      run: |
        mkdir -p performance/baselines
        mkdir -p performance/current  
        mkdir -p performance/reports

    - name: Run current performance benchmarks
      run: |
        echo "🚀 Running current performance benchmarks..."
        mix run bench/suites/parser/parser_benchmark.exs --json > performance/current/parser.json
        mix run bench/suites/terminal/buffer_benchmark.exs --json > performance/current/buffer.json
        mix run bench/suites/terminal/cursor_benchmark.exs --json > performance/current/cursor.json
        echo "✅ Current benchmarks complete"

    - name: Checkout baseline commit
      if: github.event_name == 'pull_request'
      run: |
        BASELINE_REF="${{ github.event.inputs.baseline_ref || 'origin/master' }}"
        echo "📊 Checking out baseline: $BASELINE_REF"
        git fetch origin
        git checkout $BASELINE_REF
        
        # Compile baseline version
        mix deps.get
        mix compile --warnings-as-errors

    - name: Run baseline performance benchmarks
      if: github.event_name == 'pull_request'
      run: |
        echo "⏱️  Running baseline performance benchmarks..."
        mix run bench/suites/parser/parser_benchmark.exs --json > performance/baselines/parser.json
        mix run bench/suites/terminal/buffer_benchmark.exs --json > performance/baselines/buffer.json  
        mix run bench/suites/terminal/cursor_benchmark.exs --json > performance/baselines/cursor.json
        echo "✅ Baseline benchmarks complete"

    - name: Return to current branch
      if: github.event_name == 'pull_request'
      run: |
        git checkout ${{ github.sha }}
        mix deps.get
        mix compile --warnings-as-errors

    - name: Download previous baseline (for push events)
      if: github.event_name == 'push'
      continue-on-error: true
      uses: actions/cache@v4
      with:
        path: performance/baselines
        key: performance-baseline-${{ github.ref_name }}-${{ github.sha }}
        restore-keys: |
          performance-baseline-${{ github.ref_name }}-
          performance-baseline-master-

    - name: Generate performance regression report
      run: |
        echo "📈 Generating performance regression report..."
        
        # Create analysis script
        cat > analyze_performance.exs << 'EOF'
        defmodule PerformanceAnalyzer do
          def analyze do
            current_parser = load_benchmark("performance/current/parser.json")
            current_buffer = load_benchmark("performance/current/buffer.json")
            current_cursor = load_benchmark("performance/current/cursor.json")
            
            baseline_parser = load_benchmark("performance/baselines/parser.json")
            baseline_buffer = load_benchmark("performance/baselines/buffer.json")
            baseline_cursor = load_benchmark("performance/baselines/cursor.json")
            
            results = %{
              parser: compare_benchmarks(baseline_parser, current_parser, "Parser"),
              buffer: compare_benchmarks(baseline_buffer, current_buffer, "Buffer"),
              cursor: compare_benchmarks(baseline_cursor, current_cursor, "Cursor")
            }
            
            generate_report(results)
            
            # Exit with error if significant regressions found
            if has_regressions?(results) do
              System.halt(1)
            end
          end
          
          defp load_benchmark(path) do
            case File.read(path) do
              {:ok, content} -> Jason.decode!(content)
              {:error, _} -> %{}
            end
          end
          
          defp compare_benchmarks(baseline, current, module_name) when baseline == %{} do
            IO.puts("⚠️  No baseline data for #{module_name}, skipping comparison")
            %{status: :no_baseline, regressions: [], improvements: []}
          end
          
          defp compare_benchmarks(baseline, current, module_name) do
            IO.puts("🔍 Analyzing #{module_name} performance...")
            
            regressions = []
            improvements = []
            
            # Compare key metrics
            parser_time_regression = compare_metric(
              baseline["statistics"]["average"], 
              current["statistics"]["average"],
              "average_time"
            )
            
            parser_memory_regression = compare_metric(
              baseline["statistics"].get("memory", 0),
              current["statistics"].get("memory", 0), 
              "memory_usage"
            )
            
            regressions = if parser_time_regression.regression?, 
              do: [parser_time_regression | regressions], else: regressions
            improvements = if parser_time_regression.improvement?,
              do: [parser_time_regression | improvements], else: improvements
              
            regressions = if parser_memory_regression.regression?,
              do: [parser_memory_regression | regressions], else: regressions
            improvements = if parser_memory_regression.improvement?,
              do: [parser_memory_regression | improvements], else: improvements
            
            %{
              status: if(length(regressions) > 0, do: :regressions, else: :ok),
              regressions: regressions,
              improvements: improvements
            }
          end
          
          defp compare_metric(baseline, current, metric_name) do
            if baseline && current do
              change_ratio = (current - baseline) / baseline
              change_percent = change_ratio * 100
              
              cond do
                change_ratio > 0.05 ->  # >5% slower is regression
                  %{
                    regression?: true,
                    improvement?: false,
                    metric: metric_name,
                    baseline: baseline,
                    current: current,
                    change_percent: change_percent
                  }
                change_ratio < -0.05 ->  # >5% faster is improvement
                  %{
                    regression?: false,
                    improvement?: true,
                    metric: metric_name,
                    baseline: baseline,
                    current: current,
                    change_percent: change_percent
                  }
                true ->  # Within 5% tolerance
                  %{
                    regression?: false,
                    improvement?: false,
                    metric: metric_name,
                    baseline: baseline,
                    current: current,
                    change_percent: change_percent
                  }
              end
            else
              %{
                regression?: false,
                improvement?: false,
                metric: metric_name,
                baseline: baseline,
                current: current,
                change_percent: 0
              }
            end
          end
          
          defp generate_report(results) do
            IO.puts("\n📊 Performance Regression Report")
            IO.puts("================================")
            
            total_regressions = count_total(results, :regressions)
            total_improvements = count_total(results, :improvements)
            
            IO.puts("Summary:")
            IO.puts("  🔴 Regressions: #{total_regressions}")
            IO.puts("  🟢 Improvements: #{total_improvements}")
            
            for {module, result} <- results do
              IO.puts("\n#{String.capitalize(Atom.to_string(module))} Module:")
              
              case result.regressions do
                [] -> IO.puts("  ✅ No regressions detected")
                regressions ->
                  IO.puts("  🔴 #{length(regressions)} regression(s):")
                  for reg <- regressions do
                    IO.puts("    - #{reg.metric}: #{Float.round(reg.change_percent, 2)}% slower")
                  end
              end
              
              case result.improvements do
                [] -> nil
                improvements ->
                  IO.puts("  🟢 #{length(improvements)} improvement(s):")
                  for imp <- improvements do
                    IO.puts("    - #{imp.metric}: #{abs(Float.round(imp.change_percent, 2))}% faster")
                  end
              end
            end
            
            # Save detailed report
            report_content = Jason.encode!(%{
              timestamp: DateTime.utc_now() |> DateTime.to_iso8601(),
              summary: %{
                total_regressions: total_regressions,
                total_improvements: total_improvements
              },
              modules: results
            }, pretty: true)
            
            File.write!("performance/reports/regression_report.json", report_content)
            IO.puts("\n📝 Detailed report saved to performance/reports/regression_report.json")
          end
          
          defp count_total(results, key) do
            results
            |> Enum.map(fn {_, result} -> length(result[key]) end)
            |> Enum.sum()
          end
          
          defp has_regressions?(results) do
            count_total(results, :regressions) > 0
          end
        end
        
        PerformanceAnalyzer.analyze()
        EOF
        
        # Install Jason for JSON parsing
        mix archive.install hex jason 1.4.4 --force
        
        # Run analysis
        elixir analyze_performance.exs

    - name: Upload performance artifacts
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: performance-results-${{ matrix.elixir }}-${{ matrix.otp }}
        path: |
          performance/current/
          performance/baselines/
          performance/reports/
        retention-days: 30

    - name: Save baseline for future comparisons
      if: github.event_name == 'push' && github.ref == 'refs/heads/master'
      uses: actions/cache@v4
      with:
        path: performance/baselines
        key: performance-baseline-${{ github.ref_name }}-${{ github.sha }}

    - name: Comment performance results on PR
      if: github.event_name == 'pull_request' && always()
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');
          
          let reportContent = "## 📊 Performance Regression Test Results\n\n";
          
          try {
            if (fs.existsSync('performance/reports/regression_report.json')) {
              const report = JSON.parse(fs.readFileSync('performance/reports/regression_report.json', 'utf8'));
              
              reportContent += `**Summary:**\n`;
              reportContent += `- 🔴 Regressions: ${report.summary.total_regressions}\n`;
              reportContent += `- 🟢 Improvements: ${report.summary.total_improvements}\n\n`;
              
              for (const [module, results] of Object.entries(report.modules)) {
                reportContent += `**${module.charAt(0).toUpperCase() + module.slice(1)} Module:**\n`;
                
                if (results.regressions.length > 0) {
                  reportContent += `🔴 Regressions:\n`;
                  for (const reg of results.regressions) {
                    reportContent += `- ${reg.metric}: ${reg.change_percent.toFixed(2)}% slower\n`;
                  }
                }
                
                if (results.improvements.length > 0) {
                  reportContent += `🟢 Improvements:\n`;
                  for (const imp of results.improvements) {
                    reportContent += `- ${imp.metric}: ${Math.abs(imp.change_percent).toFixed(2)}% faster\n`;
                  }
                }
                
                if (results.regressions.length === 0 && results.improvements.length === 0) {
                  reportContent += `✅ No significant changes\n`;
                }
                
                reportContent += `\n`;
              }
              
              if (report.summary.total_regressions > 0) {
                reportContent += `\n⚠️ **Performance regressions detected!** Please review the changes.\n`;
                reportContent += `\nPerformance targets (from TODO.md):\n`;
                reportContent += `- Parser: <3μs average\n`;
                reportContent += `- Render: <1ms average\n`;  
                reportContent += `- Memory: <3MB per session\n`;
              }
              
            } else {
              reportContent += "❌ Could not generate performance report. Check workflow logs.\n";
            }
          } catch (error) {
            reportContent += `❌ Error reading performance report: ${error.message}\n`;
          }
          
          // Find existing comment
          const { data: comments } = await github.rest.issues.listComments({
            owner: context.repo.owner,
            repo: context.repo.repo,
            issue_number: context.issue.number
          });
          
          const existingComment = comments.find(comment => 
            comment.body.includes('Performance Regression Test Results')
          );
          
          if (existingComment) {
            // Update existing comment
            await github.rest.issues.updateComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              comment_id: existingComment.id,
              body: reportContent
            });
          } else {
            // Create new comment
            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
              body: reportContent
            });
          }

  benchmark-collection:
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Elixir
      uses: erlef/setup-beam@v1
      with:
        elixir-version: '1.17.1'
        otp-version: '25.3.2.7'

    - name: Install dependencies
      run: |
        mix local.hex --force
        mix local.rebar --force
        mix deps.get
        mix compile

    - name: Run comprehensive benchmarks
      run: |
        mkdir -p benchmarks/weekly
        
        echo "📊 Running comprehensive weekly benchmarks..."
        mix run bench/suites/core/performance_summary.exs --json > benchmarks/weekly/$(date +%Y-%m-%d).json
        
        echo "✅ Weekly benchmarks completed"

    - name: Upload weekly benchmarks
      uses: actions/upload-artifact@v4
      with:
        name: weekly-benchmarks-$(date +%Y-%m-%d)
        path: benchmarks/weekly/
        retention-days: 90